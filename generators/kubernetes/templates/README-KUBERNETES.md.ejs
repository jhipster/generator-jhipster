# JHipster-generated Kubernetes configuration

## Preparation

You will need to push your image to a registry. If you have not done so, use the following commands to tag and push the images:

```
<%_ for (let i = 0; i < appsFolders.length; i++) { _%>
    <%_ if (appConfigs[i].baseName.toLowerCase() !== appConfigs[i].targetImageName) { _%>
$ docker image tag <%= appConfigs[i].baseName.toLowerCase() %> <%= appConfigs[i].targetImageName %>
    <%_ } _%>
$ <%= dockerPushCommand %> <%= appConfigs[i].targetImageName %>
<%_ } _%>
```

## Deployment

You can deploy all your apps by running the below bash command:

```
./kubectl-apply.sh
```

## Exploring your services


<%_ if (gatewayNb + monolithicNb >= 1) { _%>
Use these commands to find your application's IP addresses:

```
<%_ for (let i = 0; i < appsFolders.length; i++) { _%>
<%_ if (appConfigs[i].applicationType === 'gateway' || appConfigs[i].applicationType === 'monolith') { _%>
$ kubectl get svc <%= appConfigs[i].baseName.toLowerCase() %><%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
        <%_ } _%>
    <%_ } _%>
<%_ } _%>
```

## Scaling your deployments

You can scale your apps using

```
$ kubectl scale deployment <app-name> --replicas <replica-count><%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

## zero-downtime deployments

The default way to update a running app in kubernetes, is to deploy a new image tag to your docker registry and then deploy it using

```
$ kubectl set image deployment/<app-name>-app <app-name>=<new-image> <%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

Using livenessProbes and readinessProbe allow you to tell Kubernetes about the state of your applications, in order to ensure availablity of your services. You will need minimum 2 replicas for every application deployment if you want to have zero-downtime deployed. This is because the rolling upgrade strategy first kills a running replica in order to place a new. Running only one replica, will cause a short downtime during upgrades.
<%_ if (monitoring === 'elk' || monitoring === 'prometheus') { _%>

## Monitoring tools

<%_ if (monitoring === 'elk') { _%>
### JHipster console

Your application logs can be found in JHipster console (powered by Kibana). You can find its service details by
```
$ kubectl get svc jhipster-console<%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

* If you have chosen *Ingress*, then you should be able to access Kibana using the given ingress domain.
* If you have chosen *NodePort*, then point your browser to an IP of any of your nodes and use the node port described in the output.
* If you have chosen *LoadBalancer*, then use the IaaS provided LB IP
<%_ } _%>
<%_ if (monitoring === 'prometheus') { _%>

### Prometheus metrics

Generator is also packaged with [Prometheus operator by CoreOS](https://github.com/coreos/prometheus-operator).

**hint**: use must build your apps with `prometheus` profile active!

Application metrics can be explored in Prometheus through,

```
$ kubectl get svc jhipster-prometheus<%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

Also the visualisation can be explored in Grafana which is pre-configured with a dashboard view. You can find the service details by
```
$ kubectl get svc jhipster-grafana<%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

* If you have chosen *Ingress*, then you should be able to access Grafana using the given ingress domain.
* If you have chosen *NodePort*, then point your browser to an IP of any of your nodes and use the node port described in the output.
* If you have chosen *LoadBalancer*, then use the IaaS provided LB IP

<%_ } _%>
<%_ } _%>

<%_ if (app.serviceDiscoveryType === 'eureka') { _%>
## JHipster registry

The registry is deployed using a headless service in kubernetes, so the primary service has no IP address, and cannot get a node port. You can create a secondary service for any type, using:

```
$ kubectl expose service jhipster-registry --type=NodePort --name=exposed-registry<%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

and explore the details using

```
$ kubectl get svc exposed-registry<%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```

For scaling the JHipster registry, use

```
$ kubectl scale statefulset jhipster-registry --replicas 3<%= kubernetesNamespace === 'default' ? '' : ` -n ${kubernetesNamespace}` %>
```
<%_ } _%>


## Troubleshooting

> my apps doesn't get pulled, because of 'imagePullBackof'

Check the registry your Kubernetes cluster is accessing. If you are using a private registry, you should add it to your namespace by `kubectl create secret docker-registry` (check the [docs](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) for more info)

> my applications get killed, before they can boot up

This can occur if your cluster has low resource (e.g. Minikube). Increase the `initialDelySeconds` value of livenessProbe of your deployments

> my applications are starting very slow, despite I have a cluster with many resources

The default setting are optimized for middle-scale clusters. You are free to increase the JAVA_OPTS environment variable, and resource requests and limits to improve the performance. Be careful!

<%_ if (monitoring === 'prometheus') { _%>
> I have selected Prometheus but no targets are visible

This depends on the setup of the Prometheus operator and the access control policies in your cluster. Version 1.6.0+ is needed for the RBAC setup to work.

> I have selected Prometheus, but my targets never get scraped

This means your applications are probably not built using the `prometheus` profile in Maven/Gradle
<%_ } _%>

> my SQL-based microservice is stuck during Liquibase initialization when running multiple replicas

Sometimes the database changelog lock gets corrupted. You will need to connect to the database using `kubectl exec -it` and remove all lines of liquibases `databasechangeloglock` table.
